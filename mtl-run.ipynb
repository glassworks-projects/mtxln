{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import nlp\n",
    "from transformers import XLNetTokenizer, XLNetModel,AutoTokenizer\n",
    "import dataclasses\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from transformers.training_args import is_tpu_available\n",
    "from transformers.trainer import get_tpu_sampler\n",
    "from transformers.data.data_collator import DataCollator, InputDataClass\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "from typing import List, Union, Dict\n",
    "from time import time\n",
    "import pickle \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globals\n",
    "\n",
    "TASKS = ['cola', 'stsb', 'wnli']\n",
    "\n",
    "# modify this if you're encountering memory errors with your hardware setup\n",
    "max_length = 340\n",
    "\n",
    "# to run with BERT, swap the commented and uncommented lines below for model_class\n",
    "# model_class = 'bert'\n",
    "model_class = 'xlnet'\n",
    "model_name = '%s-base-cased' % model_class\n",
    "\n",
    "logpath = './mtl_%s_logs' % model_class\n",
    "\n",
    "if not os.path.exists(logpath) :\n",
    "    os.makedirs(logpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = {task : nlp.load_dataset('glue', name=task) for task in TASKS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_name, dataset in dataset_dict.items():\n",
    "    print(task_name)\n",
    "    print(dataset_dict[task_name][\"train\"][5])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskModel(transformers.PreTrainedModel):\n",
    "    def __init__(self, encoder, taskmodels_dict):\n",
    "        \"\"\"\n",
    "        Setting MultitaskModel up as a PretrainedModel allows us\n",
    "        to take better advantage of Trainer features\n",
    "        \"\"\"\n",
    "        super().__init__(transformers.PretrainedConfig())\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.taskmodels_dict = nn.ModuleDict(taskmodels_dict)\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, model_name, model_type_dict, model_config_dict=None):\n",
    "        \"\"\"\n",
    "        This creates a MultitaskModel using the model class and config objects\n",
    "        from single-task models. \n",
    "\n",
    "        We do this by creating each single-task model, and having them share\n",
    "        the same encoder transformer.\n",
    "        \"\"\"\n",
    "        shared_encoder = None\n",
    "        taskmodels_dict = {}\n",
    "        for task_name, model_type in model_type_dict.items():\n",
    "            model = model_type.from_pretrained(\n",
    "                model_name, \n",
    "                config=model_config_dict[task_name],\n",
    "            )\n",
    "            if shared_encoder is None:\n",
    "                print('*****************')\n",
    "                #print(cls.get_encoder_attr_name(model))\n",
    "                \n",
    "                shared_encoder = getattr(model, model.base_model_prefix)\n",
    "                #shared_encoder\n",
    "                print(shared_encoder)\n",
    "                print('*****************')\n",
    "            else:\n",
    "                setattr(model, model.base_model_prefix\n",
    "                        , shared_encoder)\n",
    "            taskmodels_dict[task_name] = model\n",
    "        return cls(encoder=shared_encoder, taskmodels_dict=taskmodels_dict)\n",
    "\n",
    "\n",
    "    def forward(self, task_name, **kwargs):\n",
    "        return self.taskmodels_dict[task_name](**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_type_dict and model_config_dict will need to be manually updated if you change the tasks to train on, \n",
    "# depending on the type of tasks you choose\n",
    "\n",
    "multitask_model = MultitaskModel.create(\n",
    "    model_name=model_name,\n",
    "    model_type_dict={\n",
    "        \"stsb\": transformers.AutoModelForSequenceClassification,\n",
    "        \"cola\": transformers.AutoModelForSequenceClassification,\n",
    "        \"wnli\": transformers.AutoModelForSequenceClassification\n",
    "    },\n",
    "    model_config_dict={\n",
    "        \"stsb\": transformers.AutoConfig.from_pretrained(model_name, num_labels=1),\n",
    "        \"cola\": transformers.AutoConfig.from_pretrained(model_name, num_labels=2),\n",
    "        \"wnli\": transformers.AutoConfig.from_pretrained(model_name, num_labels=2)\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_sst2_features(example_batch):\n",
    "    inputs = example_batch['sentence']\n",
    "    features = tokenizer.batch_encode_plus(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        pad_to_max_length=True\n",
    "    )\n",
    "    \n",
    "    features['labels'] = example_batch['label']\n",
    "    return features\n",
    "\n",
    "def convert_to_cola_features(example_batch):\n",
    "    inputs = example_batch['sentence']\n",
    "    features = tokenizer.batch_encode_plus(\n",
    "        inputs, max_length=max_length, pad_to_max_length=True\n",
    "    )\n",
    "    features[\"labels\"] = example_batch[\"label\"]\n",
    "    return features\n",
    "\n",
    "def convert_to_qnli_features(example_batch):\n",
    "    inputs = list(zip(example_batch['question'], example_batch['sentence']))\n",
    "    features = tokenizer.batch_encode_plus(\n",
    "        inputs, max_length=max_length, pad_to_max_length=True\n",
    "    )\n",
    "    features[\"labels\"] = example_batch[\"label\"]\n",
    "    return features\n",
    "\n",
    "def convert_to_wnli_features(example_batch):\n",
    "    inputs = list(zip(example_batch['sentence1'], example_batch['sentence2']))\n",
    "    features = tokenizer.batch_encode_plus(\n",
    "        inputs, max_length=max_length, pad_to_max_length=True\n",
    "    )\n",
    "    features[\"labels\"] = example_batch[\"label\"]\n",
    "    return features\n",
    "\n",
    "def convert_to_mnli_features(example_batch):\n",
    "    inputs = list(zip(example_batch['premise'], example_batch['hypothesis']))\n",
    "    features = tokenizer.batch_encode_plus(\n",
    "        inputs, max_length=max_length, pad_to_max_length=True\n",
    "    )\n",
    "    features[\"labels\"] = example_batch[\"label\"]\n",
    "    return features\n",
    "\n",
    "def convert_to_stsb_features(example_batch):\n",
    "    inputs = list(zip(example_batch['sentence1'], example_batch['sentence2']))\n",
    "    features = tokenizer.batch_encode_plus(\n",
    "        inputs, max_length=max_length, pad_to_max_length=True\n",
    "    )\n",
    "    features[\"labels\"] = example_batch[\"label\"]\n",
    "    return features\n",
    "\n",
    "def convert_to_rte_features(example_batch):\n",
    "    inputs = list(zip(example_batch['sentence1'], example_batch['sentence2']))\n",
    "    features = tokenizer.batch_encode_plus(\n",
    "        inputs, max_length=max_length, pad_to_max_length=True\n",
    "    )\n",
    "    features[\"labels\"] = example_batch[\"label\"]\n",
    "    return features\n",
    "\n",
    "def convert_to_commonsense_qa_features(example_batch):\n",
    "    num_examples = len(example_batch[\"question\"])\n",
    "    num_choices = len(example_batch[\"choices\"][0][\"text\"])\n",
    "    features = {}\n",
    "    for example_i in range(num_examples):\n",
    "        choices_inputs = tokenizer.batch_encode_plus(\n",
    "            list(zip(\n",
    "                [example_batch[\"question\"][example_i]] * num_choices,\n",
    "                example_batch[\"choices\"][example_i][\"text\"],\n",
    "            )),\n",
    "            max_length=max_length, pad_to_max_length=True,\n",
    "        )\n",
    "        for k, v in choices_inputs.items():\n",
    "            if k not in features:\n",
    "                features[k] = []\n",
    "            features[k].append(v)\n",
    "    labels2id = {char: i for i, char in enumerate(\"ABCDE\")}\n",
    "    # Dummy answers for test\n",
    "    if example_batch[\"answerKey\"][0]:\n",
    "        features[\"labels\"] = [labels2id[ans] for ans in example_batch[\"answerKey\"]]\n",
    "    else:\n",
    "        features[\"labels\"] = [0] * num_examples    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can write convert_func_dict manually using the above functions if this comprehension gives you trouble\n",
    "local_caller = locals()\n",
    "\n",
    "convert_func_dict = {task : local_caller['convert_to_%s_features' % task] for task in TASKS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name,do_lower_case=False)\n",
    "\n",
    "columns_dict = {task : ['input_ids', 'attention_mask', 'labels'] for task in TASKS}\n",
    "\n",
    "features_dict = {}\n",
    "for task_name, dataset in dataset_dict.items():\n",
    "    features_dict[task_name] = {}\n",
    "    for phase, phase_dataset in dataset.items():\n",
    "        features_dict[task_name][phase] = phase_dataset.map(\n",
    "            convert_func_dict[task_name],\n",
    "            batched=True,\n",
    "            load_from_cache_file=False,\n",
    "        )\n",
    "        print(task_name, phase, len(phase_dataset), len(features_dict[task_name][phase]))\n",
    "        features_dict[task_name][phase].set_format(\n",
    "            type=\"torch\", \n",
    "            columns=columns_dict[task_name],\n",
    "        )\n",
    "        print(task_name, phase, len(phase_dataset), len(features_dict[task_name][phase]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPDataCollator(DataCollator):\n",
    "    \"\"\"\n",
    "    Extending the existing DataCollator to work with NLP dataset batches\n",
    "    \"\"\"\n",
    "    def collate_batch(self, features: List[Union[InputDataClass, Dict]]) -> Dict[str, torch.Tensor]:\n",
    "        first = features[0]\n",
    "        if isinstance(first, dict):\n",
    "          # NLP data sets current works presents features as lists of dictionary\n",
    "          # (one per example), so we  will adapt the collate_batch logic for that\n",
    "            if \"labels\" in first and first[\"labels\"] is not None:\n",
    "                if first[\"labels\"].dtype == torch.int64:\n",
    "                    labels = torch.tensor([f[\"labels\"] for f in features], dtype=torch.long)\n",
    "                else:\n",
    "                    labels = torch.tensor([f[\"labels\"] for f in features], dtype=torch.float)\n",
    "                batch = {\"labels\": labels}\n",
    "            for k, v in first.items():\n",
    "                if k != \"labels\" and v is not None and not isinstance(v, str):\n",
    "                    batch[k] = torch.stack([f[k] for f in features])\n",
    "            return batch\n",
    "        else:\n",
    "          # otherwise, revert to using the default collate_batch\n",
    "          return DefaultDataCollator().collate_batch(features)\n",
    "\n",
    "\n",
    "class StrIgnoreDevice(str):\n",
    "    \"\"\"\n",
    "    This is a hack. The Trainer is going call .to(device) on every input\n",
    "    value, but we need to pass in an additional `task_name` string.\n",
    "    This prevents it from throwing an error\n",
    "    \"\"\"\n",
    "    def to(self, device):\n",
    "        return self\n",
    "\n",
    "\n",
    "class DataLoaderWithTaskname:\n",
    "    \"\"\"\n",
    "    Wrapper around a DataLoader to also yield a task name\n",
    "    \"\"\"\n",
    "    def __init__(self, task_name, data_loader):\n",
    "        self.task_name = task_name\n",
    "        self.data_loader = data_loader\n",
    "\n",
    "        self.batch_size = data_loader.batch_size\n",
    "        self.dataset = data_loader.dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_loader)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.data_loader:\n",
    "            batch[\"task_name\"] = StrIgnoreDevice(self.task_name)\n",
    "            yield batch\n",
    "\n",
    "\n",
    "class MultitaskDataloader:\n",
    "    \"\"\"\n",
    "    Data loader that combines and samples from multiple single-task\n",
    "    data loaders.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataloader_dict):\n",
    "        self.dataloader_dict = dataloader_dict\n",
    "        self.num_batches_dict = {\n",
    "            task_name: len(dataloader) \n",
    "            for task_name, dataloader in self.dataloader_dict.items()\n",
    "        }\n",
    "        self.task_name_list = list(self.dataloader_dict)\n",
    "        self.dataset = [None] * sum(\n",
    "            len(dataloader.dataset) \n",
    "            for dataloader in self.dataloader_dict.values()\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(self.num_batches_dict.values())\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        For each batch, sample a task, and yield a batch from the respective\n",
    "        task Dataloader.\n",
    "\n",
    "        We use size-proportional sampling, but you could easily modify this\n",
    "        to sample from some-other distribution.\n",
    "        \"\"\"\n",
    "        task_choice_list = []\n",
    "        for i, task_name in enumerate(self.task_name_list):\n",
    "            task_choice_list += [i] * self.num_batches_dict[task_name]\n",
    "        task_choice_list = np.array(task_choice_list)\n",
    "        np.random.shuffle(task_choice_list)\n",
    "        dataloader_iter_dict = {\n",
    "            task_name: iter(dataloader) \n",
    "            for task_name, dataloader in self.dataloader_dict.items()\n",
    "        }\n",
    "        for task_choice in task_choice_list:\n",
    "            task_name = self.task_name_list[task_choice]\n",
    "            yield next(dataloader_iter_dict[task_name])    \n",
    "\n",
    "class MultitaskTrainer(transformers.Trainer):\n",
    "\n",
    "    def get_single_train_dataloader(self, task_name, train_dataset):\n",
    "        \"\"\"\n",
    "        Create a single-task data loader that also yields task names\n",
    "        \"\"\"\n",
    "        if self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "        if is_tpu_available():\n",
    "            train_sampler = get_tpu_sampler(train_dataset)\n",
    "        else:\n",
    "            train_sampler = (\n",
    "                RandomSampler(train_dataset)\n",
    "                if self.args.local_rank == -1\n",
    "                else DistributedSampler(train_dataset)\n",
    "            )\n",
    "\n",
    "        data_loader = DataLoaderWithTaskname(\n",
    "            task_name=task_name,\n",
    "            data_loader=DataLoader(\n",
    "              train_dataset,\n",
    "              batch_size=self.args.train_batch_size,\n",
    "              sampler=train_sampler,\n",
    "              collate_fn=self.data_collator.collate_batch,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if is_tpu_available():\n",
    "            data_loader = pl.ParallelLoader(\n",
    "                data_loader, [self.args.device]\n",
    "            ).per_device_loader(self.args.device)\n",
    "        return data_loader\n",
    "\n",
    "    def get_train_dataloader(self):\n",
    "        \"\"\"\n",
    "        Returns a MultitaskDataloader, which is not actually a Dataloader\n",
    "        but an iterable that returns a generator that samples from each \n",
    "        task Dataloader\n",
    "        \"\"\"\n",
    "        return MultitaskDataloader({\n",
    "            task_name: self.get_single_train_dataloader(task_name, task_dataset)\n",
    "            for task_name, task_dataset in self.train_dataset.items()\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time()\n",
    "print('Training started at ',start)\n",
    "train_dataset = {\n",
    "    task_name: dataset[\"train\"] \n",
    "    for task_name, dataset in features_dict.items()\n",
    "}\n",
    "trainer = MultitaskTrainer(\n",
    "    model=multitask_model,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=\"./models/mtl_%s\" % model_class,\n",
    "        overwrite_output_dir=True,\n",
    "        learning_rate=5e-5,\n",
    "        do_train=True,\n",
    "        num_train_epochs=3,\n",
    "        # Adjust batch size if this doesn't fit on the Colab GPU\n",
    "        per_device_train_batch_size=8,  \n",
    "        save_steps=3000,\n",
    "        logging_steps=100,\n",
    "        logging_dir=logpath\n",
    "    ),\n",
    "    data_collator=NLPDataCollator(),\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "trainer.train()\n",
    "print(time()-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_dict = {}\n",
    "for task_name in TASKS:\n",
    "    eval_dataloader = DataLoaderWithTaskname(\n",
    "        task_name,\n",
    "        trainer.get_eval_dataloader(eval_dataset=features_dict[task_name][\"validation\"])\n",
    "    )\n",
    "    print(eval_dataloader.data_loader.collate_fn)\n",
    "    preds_dict[task_name] = trainer._prediction_loop(\n",
    "        eval_dataloader, \n",
    "        description=f\"Validation: {task_name}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score computation will need to be edit manually if you change the tasks, as the method of computing score \n",
    "# varies between tasks\n",
    "\n",
    "scores = {}\n",
    "\n",
    "scores['cola'] = nlp.load_metric('glue', name='cola').compute(\n",
    "    np.argmax(preds_dict['cola'].predictions, axis=1),\n",
    "    preds_dict['cola'].label_ids\n",
    ")\n",
    "scores['stsb'] = nlp.load_metric('glue', name=\"stsb\").compute(\n",
    "    preds_dict[\"stsb\"].predictions.flatten(),\n",
    "    preds_dict[\"stsb\"].label_ids,\n",
    ")\n",
    "scores['wnli'] = nlp.load_metric('glue', name='wnli').compute(\n",
    "    np.argmax(preds_dict['wnli'].predictions, axis=1),\n",
    "    preds_dict['wnli'].label_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mtl_%s_scores.pkl' % model_class, 'wb') as fd :\n",
    "    pickle.dump(scores, fd)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-6.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-6:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
