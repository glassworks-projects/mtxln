{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import nlp\n",
    "from transformers import AutoTokenizer\n",
    "import dataclasses\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from transformers.training_args import is_tpu_available\n",
    "from transformers.trainer import get_tpu_sampler\n",
    "from transformers.data.data_collator import DataCollator, InputDataClass\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "from typing import List, Union, Dict\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globals\n",
    "\n",
    "TASKS = ['rte', 'wnli']\n",
    "\n",
    "# modify this if you're encountering memory errors with your hardware setup\n",
    "max_length = 340\n",
    "\n",
    "# to run with BERT, swap the commented and uncommented lines below for model_class\n",
    "# model_class = 'bert'\n",
    "model_class = 'xlnet'\n",
    "model_name = '%s-base-cased' % model_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = {task : nlp.load_dataset('glue', name=task) for task in TASKS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskModel(transformers.PreTrainedModel):\n",
    "    def __init__(self, encoder, taskmodels_dict):\n",
    "        \"\"\"\n",
    "        Setting MultitaskModel up as a PretrainedModel allows us\n",
    "        to take better advantage of Trainer features\n",
    "        \"\"\"\n",
    "        super().__init__(transformers.PretrainedConfig(max_length=max_length))\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.taskmodels_dict = nn.ModuleDict(taskmodels_dict)\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, model_name, model_type_dict, model_config_dict=None):\n",
    "        \"\"\"\n",
    "        This creates a MultitaskModel using the model class and config objects\n",
    "        from single-task models. \n",
    "\n",
    "        We do this by creating each single-task model, and having them share\n",
    "        the same encoder transformer.\n",
    "        \"\"\"\n",
    "        shared_encoder = None\n",
    "        taskmodels_dict = {}\n",
    "        for task_name, model_type in model_type_dict.items():\n",
    "            model = model_type.from_pretrained(\n",
    "                model_name, \n",
    "                config=model_config_dict[task_name],\n",
    "            )\n",
    "            if shared_encoder is None:\n",
    "                shared_encoder = getattr(model, model.base_model_prefix)                \n",
    "            else:\n",
    "                setattr(model, model.base_model_prefix\n",
    "                        , shared_encoder)\n",
    "            taskmodels_dict[task_name] = model\n",
    "        return cls(encoder=shared_encoder, taskmodels_dict=taskmodels_dict)\n",
    "\n",
    "\n",
    "    def forward(self, task_name, **kwargs):\n",
    "        return self.taskmodels_dict[task_name](**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_wnli_features(example_batch):\n",
    "    #print(example_batch)\n",
    "    inputs = list(zip(example_batch['sentence1'], example_batch['sentence2']))\n",
    "    features = tokenizer.batch_encode_plus(\n",
    "        inputs, max_length=max_length, pad_to_max_length=True\n",
    "    )\n",
    "    features[\"labels\"] = example_batch[\"label\"]\n",
    "    return features\n",
    "\n",
    "def convert_to_rte_features(example_batch):\n",
    "    inputs = list(zip(example_batch['sentence1'], example_batch['sentence2']))\n",
    "    features = tokenizer.batch_encode_plus(\n",
    "        inputs, max_length=max_length, pad_to_max_length=True\n",
    "    )\n",
    "    features[\"labels\"] = example_batch[\"label\"]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPDataCollator(DataCollator):\n",
    "    \"\"\"\n",
    "    Extending the existing DataCollator to work with NLP dataset batches\n",
    "    \"\"\"\n",
    "    def collate_batch(self, features: List[Union[InputDataClass, Dict]]) -> Dict[str, torch.Tensor]:\n",
    "        first = features[0]\n",
    "        if isinstance(first, dict):\n",
    "          # NLP data sets current works presents features as lists of dictionary\n",
    "          # (one per example), so we  will adapt the collate_batch logic for that\n",
    "            if \"labels\" in first and first[\"labels\"] is not None:\n",
    "                if first[\"labels\"].dtype == torch.int64:\n",
    "                    labels = torch.tensor([f[\"labels\"] for f in features], dtype=torch.long)\n",
    "                else:\n",
    "                    labels = torch.tensor([f[\"labels\"] for f in features], dtype=torch.float)\n",
    "                batch = {\"labels\": labels}\n",
    "            for k, v in first.items():\n",
    "                if k != \"labels\" and v is not None and not isinstance(v, str):\n",
    "                    batch[k] = torch.stack([f[k] for f in features])\n",
    "            return batch\n",
    "        else:\n",
    "          # otherwise, revert to using the default collate_batch\n",
    "          return DefaultDataCollator().collate_batch(features)\n",
    "\n",
    "\n",
    "class StrIgnoreDevice(str):\n",
    "    \"\"\"\n",
    "    This is a hack. The Trainer is going call .to(device) on every input\n",
    "    value, but we need to pass in an additional `task_name` string.\n",
    "    This prevents it from throwing an error\n",
    "    \"\"\"\n",
    "    def to(self, device):\n",
    "        return self\n",
    "\n",
    "\n",
    "class DataLoaderWithTaskname:\n",
    "    \"\"\"\n",
    "    Wrapper around a DataLoader to also yield a task name\n",
    "    \"\"\"\n",
    "    def __init__(self, task_name, data_loader):\n",
    "        self.task_name = task_name\n",
    "        self.data_loader = data_loader\n",
    "\n",
    "        self.batch_size = data_loader.batch_size\n",
    "        self.dataset = data_loader.dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_loader)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.data_loader:\n",
    "            batch[\"task_name\"] = StrIgnoreDevice(self.task_name)\n",
    "            yield batch\n",
    "\n",
    "\n",
    "class MultitaskDataloader:\n",
    "    \"\"\"\n",
    "    Data loader that combines and samples from multiple single-task\n",
    "    data loaders.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataloader_dict):\n",
    "        self.dataloader_dict = dataloader_dict\n",
    "        self.num_batches_dict = {\n",
    "            task_name: len(dataloader) \n",
    "            for task_name, dataloader in self.dataloader_dict.items()\n",
    "        }\n",
    "        self.task_name_list = list(self.dataloader_dict)\n",
    "        self.dataset = [None] * sum(\n",
    "            len(dataloader.dataset) \n",
    "            for dataloader in self.dataloader_dict.values()\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(self.num_batches_dict.values())\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        For each batch, sample a task, and yield a batch from the respective\n",
    "        task Dataloader.\n",
    "\n",
    "        We use size-proportional sampling, but you could easily modify this\n",
    "        to sample from some-other distribution.\n",
    "        \"\"\"\n",
    "        task_choice_list = []\n",
    "        for i, task_name in enumerate(self.task_name_list):\n",
    "            task_choice_list += [i] * self.num_batches_dict[task_name]\n",
    "        task_choice_list = np.array(task_choice_list)\n",
    "        np.random.shuffle(task_choice_list)\n",
    "        dataloader_iter_dict = {\n",
    "            task_name: iter(dataloader) \n",
    "            for task_name, dataloader in self.dataloader_dict.items()\n",
    "        }\n",
    "        for task_choice in task_choice_list:\n",
    "            task_name = self.task_name_list[task_choice]\n",
    "            yield next(dataloader_iter_dict[task_name])    \n",
    "\n",
    "class MultitaskTrainer(transformers.Trainer):\n",
    "\n",
    "    def get_single_train_dataloader(self, task_name, train_dataset):\n",
    "        \"\"\"\n",
    "        Create a single-task data loader that also yields task names\n",
    "        \"\"\"\n",
    "        if self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "        if is_tpu_available():\n",
    "            train_sampler = get_tpu_sampler(train_dataset)\n",
    "        else:\n",
    "            train_sampler = (\n",
    "                RandomSampler(train_dataset)\n",
    "                if self.args.local_rank == -1\n",
    "                else DistributedSampler(train_dataset)\n",
    "            )\n",
    "\n",
    "        data_loader = DataLoaderWithTaskname(\n",
    "            task_name=task_name,\n",
    "            data_loader=DataLoader(\n",
    "              train_dataset,\n",
    "              batch_size=self.args.train_batch_size,\n",
    "              sampler=train_sampler,\n",
    "              collate_fn=self.data_collator.collate_batch,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if is_tpu_available():\n",
    "            data_loader = pl.ParallelLoader(\n",
    "                data_loader, [self.args.device]\n",
    "            ).per_device_loader(self.args.device)\n",
    "        return data_loader\n",
    "\n",
    "    def get_train_dataloader(self):\n",
    "        \"\"\"\n",
    "        Returns a MultitaskDataloader, which is not actually a Dataloader\n",
    "        but an iterable that returns a generator that samples from each \n",
    "        task Dataloader\n",
    "        \"\"\"\n",
    "        return MultitaskDataloader({\n",
    "            task_name: self.get_single_train_dataloader(task_name, task_dataset)\n",
    "            for task_name, task_dataset in self.train_dataset.items()\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_caller = locals()\n",
    "\n",
    "convert_func_dict = {task : local_caller['convert_to_%s_features' % task] for task in TASKS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_dict = {task : ['input_ids', 'attention_mask', 'labels'] for task in TASKS}\n",
    "\n",
    "features_dict = {}\n",
    "for task_name, dataset in dataset_dict.items():\n",
    "    features_dict[task_name] = {}\n",
    "    for phase, phase_dataset in dataset.items():\n",
    "        features_dict[task_name][phase] = phase_dataset.map(\n",
    "            convert_func_dict[task_name],\n",
    "            batched=True,\n",
    "            load_from_cache_file=False,\n",
    "        )\n",
    "        features_dict[task_name][phase].set_format(\n",
    "            type=\"torch\", \n",
    "            columns=columns_dict[task_name],\n",
    "        )\n",
    "        print(task_name, phase, len(phase_dataset), len(features_dict[task_name][phase]))\n",
    "\n",
    "train_dataset = {\n",
    "task_name: dataset[\"train\"] \n",
    "for task_name, dataset in features_dict.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "scores = {\n",
    "    'rte' : {},\n",
    "    'wnli': {}\n",
    "}\n",
    "\n",
    "# This will calculate for sequence length 8, 16, ..., 128\n",
    "# Adjust loop range to fit your experiment and hardware constraints \n",
    "for i in range(3,8):\n",
    "    max_length=2**i\n",
    "\n",
    "    multitask_model = MultitaskModel.create(\n",
    "        model_name=model_name,\n",
    "        model_type_dict={\n",
    "            \"rte\": transformers.AutoModelForSequenceClassification,\n",
    "            \"wnli\": transformers.AutoModelForSequenceClassification\n",
    "        },\n",
    "        model_config_dict={\n",
    "            \"rte\": transformers.AutoConfig.from_pretrained(model_name, num_labels=2),\n",
    "            \"wnli\": transformers.AutoConfig.from_pretrained(model_name, num_labels=2)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name,do_lower_case=True)\n",
    "    \n",
    "    trainer = MultitaskTrainer(\n",
    "        model=multitask_model,\n",
    "        args=transformers.TrainingArguments(\n",
    "            output_dir=\"./models/mtl_%s_model_%i\" % (model_class, i),\n",
    "            overwrite_output_dir=True,\n",
    "            learning_rate=5e-5,\n",
    "            do_train=True,\n",
    "            num_train_epochs=3,\n",
    "            # Adjust batch size if this doesn't fit on the Colab GPU\n",
    "            per_device_train_batch_size=8,  \n",
    "            save_steps=3000,\n",
    "            logging_steps=100,\n",
    "            logging_dir='./mtl_%s_logs_%i' % (model_class, i)\n",
    "        ),\n",
    "        data_collator=NLPDataCollator(),\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "    print('Training for max_length=',2**i)\n",
    "    start=time()\n",
    "    trainer.train()\n",
    "    print('Training time=',time()-start)\n",
    "    preds_dict = {}\n",
    "    for task_name in TASKS:\n",
    "        eval_dataloader = DataLoaderWithTaskname(\n",
    "            task_name,\n",
    "            trainer.get_eval_dataloader(eval_dataset=features_dict[task_name][\"validation\"])\n",
    "        )\n",
    "        print(eval_dataloader.data_loader.collate_fn)\n",
    "        preds_dict[task_name] = trainer._prediction_loop(\n",
    "            eval_dataloader, \n",
    "            description=f\"Validation: {task_name}\",\n",
    "        )\n",
    "     \n",
    "    scores['rte'][i] = nlp.load_metric('glue', name=\"rte\").compute(\n",
    "        np.argmax(preds_dict[\"rte\"].predictions, axis=1),\n",
    "        preds_dict[\"rte\"].label_ids\n",
    "    )\n",
    "    \n",
    "    scores['wnli'][i] = nlp.load_metric('glue', name=\"wnli\").compute(\n",
    "        np.argmax(preds_dict[\"wnli\"].predictions, axis=1),\n",
    "        preds_dict[\"wnli\"].label_ids\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('seq-length-%s.pkl' % model_class, 'wb') as file :\n",
    "    pickle.dump(scores, file)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-6.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-6:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
