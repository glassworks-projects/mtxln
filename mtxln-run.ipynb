{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==2.11.0\n",
      "  Downloading transformers-2.11.0-py3-none-any.whl (674 kB)\n",
      "\u001b[K     |████████████████████████████████| 674 kB 4.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /home/ecbm4040/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages (from transformers==2.11.0) (2020.11.13)\n",
      "Requirement already satisfied: sacremoses in /home/ecbm4040/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages (from transformers==2.11.0) (0.0.43)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ecbm4040/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages (from transformers==2.11.0) (4.49.0)\n",
      "Requirement already satisfied: filelock in /home/ecbm4040/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages (from transformers==2.11.0) (3.0.12)\n",
      "Requirement already satisfied: numpy in /home/ecbm4040/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages (from transformers==2.11.0) (1.19.2)\n",
      "Requirement already satisfied: requests in /home/ecbm4040/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages (from transformers==2.11.0) (2.25.0)\n",
      "Requirement already satisfied: packaging in /home/ecbm4040/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages (from transformers==2.11.0) (20.8)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ecbm4040/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages (from packaging->transformers==2.11.0) (2.4.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ecbm4040/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages (from requests->transformers==2.11.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ecbm4040/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages (from requests->transformers==2.11.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ecbm4040/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages (from requests->transformers==2.11.0) (1.26.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ecbm4040/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages (from requests->transformers==2.11.0) (2020.12.5)\n",
      "Requirement already satisfied: click in /home/ecbm4040/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages (from sacremoses->transformers==2.11.0) (7.1.2)\n",
      "Requirement already satisfied: six in /home/ecbm4040/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages (from sacremoses->transformers==2.11.0) (1.15.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ecbm4040/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages (from transformers==2.11.0) (4.49.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ecbm4040/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages (from transformers==2.11.0) (2020.11.13)\n",
      "Requirement already satisfied: joblib in /home/ecbm4040/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages (from sacremoses->transformers==2.11.0) (1.0.0)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.94-cp38-cp38-manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 16.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers==0.7.0\n",
      "  Downloading tokenizers-0.7.0-cp38-cp38-manylinux1_x86_64.whl (7.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.5 MB 14.4 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tokenizers, sentencepiece, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.9.4\n",
      "    Uninstalling tokenizers-0.9.4:\n",
      "      Successfully uninstalled tokenizers-0.9.4\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.0.1\n",
      "    Uninstalling transformers-4.0.1:\n",
      "      Successfully uninstalled transformers-4.0.1\n",
      "Successfully installed sentencepiece-0.1.94 tokenizers-0.7.0 transformers-2.11.0\n",
      "Collecting nlp==0.2.0\n",
      "  Downloading nlp-0.2.0-py3-none-any.whl (857 kB)\n",
      "\u001b[K     |████████████████████████████████| 857 kB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: dill in /home/ecbm4040/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages (from nlp==0.2.0) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ecbm4040/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages (from nlp==0.2.0) (4.49.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ecbm4040/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages (from nlp==0.2.0) (2.25.0)\n",
      "Requirement already satisfied: numpy in /home/ecbm4040/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages (from nlp==0.2.0) (1.19.2)\n",
      "Requirement already satisfied: filelock in /home/ecbm4040/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages (from nlp==0.2.0) (3.0.12)\n",
      "Requirement already satisfied: pyarrow>=0.16.0 in /home/ecbm4040/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages (from nlp==0.2.0) (2.0.0)\n",
      "Requirement already satisfied: numpy in /home/ecbm4040/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages (from nlp==0.2.0) (1.19.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ecbm4040/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages (from requests>=2.19.0->nlp==0.2.0) (1.26.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ecbm4040/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages (from requests>=2.19.0->nlp==0.2.0) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ecbm4040/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages (from requests>=2.19.0->nlp==0.2.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ecbm4040/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages (from requests>=2.19.0->nlp==0.2.0) (2020.12.5)\n",
      "Installing collected packages: nlp\n",
      "  Attempting uninstall: nlp\n",
      "    Found existing installation: nlp 0.4.0\n",
      "    Uninstalling nlp-0.4.0:\n",
      "      Successfully uninstalled nlp-0.4.0\n",
      "Successfully installed nlp-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==2.11.0\n",
    "!pip install nlp==0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import nlp\n",
    "from transformers import XLNetTokenizer, XLNetModel,AutoTokenizer\n",
    "import dataclasses\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from transformers.training_args import is_tpu_available\n",
    "from transformers.trainer import get_tpu_sampler\n",
    "from transformers.data.data_collator import DataCollator, InputDataClass\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "from typing import List, Union, Dict\n",
    "from time import time\n",
    "# import logging\n",
    "# logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = {\n",
    "    \"cola\": nlp.load_dataset('glue', name=\"cola\"),\n",
    "    \"stsb\": nlp.load_dataset('glue', name=\"stsb\"),\n",
    "    \"wnli\": nlp.load_dataset('glue', name=\"wnli\")   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cola\n",
      "{'sentence': \"I'll fix you a drink.\", 'label': 1, 'idx': 5}\n",
      "\n",
      "stsb\n",
      "{'sentence1': 'Some men are fighting.', 'sentence2': 'Two men are fighting.', 'label': 4.25, 'idx': 5}\n",
      "\n",
      "wnli\n",
      "{'sentence1': 'George got free tickets to the play, but he gave them to Eric, because he was particularly eager to see it.', 'sentence2': 'George was particularly eager to see it.', 'label': 0, 'idx': 5}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for task_name, dataset in dataset_dict.items():\n",
    "    print(task_name)\n",
    "    print(dataset_dict[task_name][\"train\"][5])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskModel(transformers.PreTrainedModel):\n",
    "    def __init__(self, encoder, taskmodels_dict):\n",
    "        \"\"\"\n",
    "        Setting MultitaskModel up as a PretrainedModel allows us\n",
    "        to take better advantage of Trainer features\n",
    "        \"\"\"\n",
    "        super().__init__(transformers.PretrainedConfig())\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.taskmodels_dict = nn.ModuleDict(taskmodels_dict)\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, model_name, model_type_dict, model_config_dict=None):\n",
    "        \"\"\"\n",
    "        This creates a MultitaskModel using the model class and config objects\n",
    "        from single-task models. \n",
    "\n",
    "        We do this by creating each single-task model, and having them share\n",
    "        the same encoder transformer.\n",
    "        \"\"\"\n",
    "        shared_encoder = None\n",
    "        taskmodels_dict = {}\n",
    "        for task_name, model_type in model_type_dict.items():\n",
    "            model = model_type.from_pretrained(\n",
    "                model_name, \n",
    "                config=model_config_dict[task_name],\n",
    "            )\n",
    "            if shared_encoder is None:\n",
    "                print('*****************')\n",
    "                #print(cls.get_encoder_attr_name(model))\n",
    "                \n",
    "                shared_encoder = getattr(model, model.base_model_prefix)\n",
    "                #shared_encoder\n",
    "                print(shared_encoder)\n",
    "                print('*****************')\n",
    "            else:\n",
    "                setattr(model, model.base_model_prefix\n",
    "                        , shared_encoder)\n",
    "            taskmodels_dict[task_name] = model\n",
    "        return cls(encoder=shared_encoder, taskmodels_dict=taskmodels_dict)\n",
    "\n",
    "\n",
    "    def forward(self, task_name, **kwargs):\n",
    "        return self.taskmodels_dict[task_name](**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************\n",
      "XLNetModel(\n",
      "  (word_embedding): Embedding(32000, 768)\n",
      "  (layer): ModuleList(\n",
      "    (0): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (3): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (4): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (5): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (6): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (7): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (8): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (9): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (10): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (11): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "*****************\n"
     ]
    }
   ],
   "source": [
    "model_name = 'xlnet-base-cased'\n",
    "multitask_model = MultitaskModel.create(\n",
    "    model_name=model_name,\n",
    "    model_type_dict={\n",
    "        \"stsb\": transformers.AutoModelForSequenceClassification,\n",
    "        \"cola\": transformers.AutoModelForSequenceClassification,\n",
    "        \"wnli\": transformers.AutoModelForSequenceClassification,\n",
    "        #\"mnli\": transformers.AutoModelForSequenceClassification,\n",
    "    },\n",
    "    model_config_dict={\n",
    "        \"stsb\": transformers.AutoConfig.from_pretrained(model_name, num_labels=1),\n",
    "        \"cola\": transformers.AutoConfig.from_pretrained(model_name, num_labels=2),\n",
    "        \"wnli\": transformers.AutoConfig.from_pretrained(model_name, num_labels=2),\n",
    "        #\"mnli\": transformers.AutoConfig.from_pretrained(model_name, num_labels=3)\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 340\n",
    "\n",
    "def convert_to_sst2_features(example_batch):\n",
    "    inputs = list(zip(example_batch['sentence']))\n",
    "    features = tokenizer.batch_encode_plus(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        pad_to_max_length=True\n",
    "    )\n",
    "    \n",
    "    features['labels'] = example_batch['label']\n",
    "    return features\n",
    "\n",
    "def convert_to_cola_features(example_batch):\n",
    "    #print(example_batch)\n",
    "    inputs = example_batch['sentence']\n",
    "    features = tokenizer.batch_encode_plus(\n",
    "        inputs, max_length=max_length, pad_to_max_length=True\n",
    "    )\n",
    "    features[\"labels\"] = example_batch[\"label\"]\n",
    "    return features\n",
    "\n",
    "def convert_to_qnli_features(example_batch):\n",
    "    #print(example_batch)\n",
    "    inputs = list(zip(example_batch['question'], example_batch['sentence']))\n",
    "    features = tokenizer.batch_encode_plus(\n",
    "        inputs, max_length=max_length, pad_to_max_length=True\n",
    "    )\n",
    "    features[\"labels\"] = example_batch[\"label\"]\n",
    "    return features\n",
    "\n",
    "def convert_to_wnli_features(example_batch):\n",
    "    #print(example_batch)\n",
    "    inputs = list(zip(example_batch['sentence1'], example_batch['sentence2']))\n",
    "    features = tokenizer.batch_encode_plus(\n",
    "        inputs, max_length=max_length, pad_to_max_length=True\n",
    "    )\n",
    "    features[\"labels\"] = example_batch[\"label\"]\n",
    "    return features\n",
    "\n",
    "def convert_to_mnli_features(example_batch):\n",
    "    #print(example_batch)\n",
    "    inputs = list(zip(example_batch['premise'], example_batch['hypothesis']))\n",
    "    features = tokenizer.batch_encode_plus(\n",
    "        inputs, max_length=max_length, pad_to_max_length=True\n",
    "    )\n",
    "    features[\"labels\"] = example_batch[\"label\"]\n",
    "    return features\n",
    "\n",
    "def convert_to_stsb_features(example_batch):\n",
    "    inputs = list(zip(example_batch['sentence1'], example_batch['sentence2']))\n",
    "    features = tokenizer.batch_encode_plus(\n",
    "        inputs, max_length=max_length, pad_to_max_length=True\n",
    "    )\n",
    "    features[\"labels\"] = example_batch[\"label\"]\n",
    "    return features\n",
    "\n",
    "def convert_to_rte_features(example_batch):\n",
    "    inputs = list(zip(example_batch['sentence1'], example_batch['sentence2']))\n",
    "    features = tokenizer.batch_encode_plus(\n",
    "        inputs, max_length=max_length, pad_to_max_length=True\n",
    "    )\n",
    "    features[\"labels\"] = example_batch[\"label\"]\n",
    "    return features\n",
    "\n",
    "def convert_to_commonsense_qa_features(example_batch):\n",
    "    num_examples = len(example_batch[\"question\"])\n",
    "    num_choices = len(example_batch[\"choices\"][0][\"text\"])\n",
    "    features = {}\n",
    "    for example_i in range(num_examples):\n",
    "        choices_inputs = tokenizer.batch_encode_plus(\n",
    "            list(zip(\n",
    "                [example_batch[\"question\"][example_i]] * num_choices,\n",
    "                example_batch[\"choices\"][example_i][\"text\"],\n",
    "            )),\n",
    "            max_length=max_length, pad_to_max_length=True,\n",
    "        )\n",
    "        for k, v in choices_inputs.items():\n",
    "            if k not in features:\n",
    "                features[k] = []\n",
    "            features[k].append(v)\n",
    "    labels2id = {char: i for i, char in enumerate(\"ABCDE\")}\n",
    "    # Dummy answers for test\n",
    "    if example_batch[\"answerKey\"][0]:\n",
    "        features[\"labels\"] = [labels2id[ans] for ans in example_batch[\"answerKey\"]]\n",
    "    else:\n",
    "        features[\"labels\"] = [0] * num_examples    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_func_dict = {\n",
    "    \"stsb\": convert_to_stsb_features,\n",
    "    \"cola\": convert_to_cola_features,\n",
    "    \"wnli\": convert_to_wnli_features\n",
    "    #\"mnli\": convert_to_mnli_features,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:01<00:00,  8.36it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 14.62it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cola train 8551 8551\n",
      "cola train 8551 8551\n",
      "cola validation 1043 1043\n",
      "cola validation 1043 1043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 14.29it/s]\n",
      " 17%|█▋        | 1/6 [00:00<00:00,  6.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cola test 1063 1063\n",
      "cola test 1063 1063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:01<00:00,  5.09it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stsb train 5749 5749\n",
      "stsb train 5749 5749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  6.40it/s]\n",
      " 50%|█████     | 1/2 [00:00<00:00,  5.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stsb validation 1500 1500\n",
      "stsb validation 1500 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  7.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.17it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 57.80it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stsb test 1379 1379\n",
      "stsb test 1379 1379\n",
      "wnli train 635 635\n",
      "wnli train 635 635\n",
      "wnli validation 71 71\n",
      "wnli validation 71 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 1/1 [00:00<00:00, 23.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wnli test 146 146\n",
      "wnli test 146 146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('xlnet-base-cased',do_lower_case=False)\n",
    "columns_dict = {\n",
    "    \"stsb\": ['input_ids', 'attention_mask', 'labels'],\n",
    "    \"cola\": ['input_ids', 'attention_mask', 'labels'],\n",
    "    \"wnli\": ['input_ids', 'attention_mask', 'labels'],\n",
    "    #\"mnli\": ['input_ids', 'attention_mask', 'labels'],\n",
    "    \n",
    "}\n",
    "\n",
    "features_dict = {}\n",
    "for task_name, dataset in dataset_dict.items():\n",
    "    features_dict[task_name] = {}\n",
    "    for phase, phase_dataset in dataset.items():\n",
    "        features_dict[task_name][phase] = phase_dataset.map(\n",
    "            convert_func_dict[task_name],\n",
    "            batched=True,\n",
    "            load_from_cache_file=False,\n",
    "        )\n",
    "        print(task_name, phase, len(phase_dataset), len(features_dict[task_name][phase]))\n",
    "        features_dict[task_name][phase].set_format(\n",
    "            type=\"torch\", \n",
    "            columns=columns_dict[task_name],\n",
    "        )\n",
    "        print(task_name, phase, len(phase_dataset), len(features_dict[task_name][phase]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPDataCollator(DataCollator):\n",
    "    \"\"\"\n",
    "    Extending the existing DataCollator to work with NLP dataset batches\n",
    "    \"\"\"\n",
    "    def collate_batch(self, features: List[Union[InputDataClass, Dict]]) -> Dict[str, torch.Tensor]:\n",
    "        first = features[0]\n",
    "        if isinstance(first, dict):\n",
    "          # NLP data sets current works presents features as lists of dictionary\n",
    "          # (one per example), so we  will adapt the collate_batch logic for that\n",
    "            if \"labels\" in first and first[\"labels\"] is not None:\n",
    "                if first[\"labels\"].dtype == torch.int64:\n",
    "                    labels = torch.tensor([f[\"labels\"] for f in features], dtype=torch.long)\n",
    "                else:\n",
    "                    labels = torch.tensor([f[\"labels\"] for f in features], dtype=torch.float)\n",
    "                batch = {\"labels\": labels}\n",
    "            for k, v in first.items():\n",
    "                if k != \"labels\" and v is not None and not isinstance(v, str):\n",
    "                    batch[k] = torch.stack([f[k] for f in features])\n",
    "            return batch\n",
    "        else:\n",
    "          # otherwise, revert to using the default collate_batch\n",
    "          return DefaultDataCollator().collate_batch(features)\n",
    "\n",
    "\n",
    "class StrIgnoreDevice(str):\n",
    "    \"\"\"\n",
    "    This is a hack. The Trainer is going call .to(device) on every input\n",
    "    value, but we need to pass in an additional `task_name` string.\n",
    "    This prevents it from throwing an error\n",
    "    \"\"\"\n",
    "    def to(self, device):\n",
    "        return self\n",
    "\n",
    "\n",
    "class DataLoaderWithTaskname:\n",
    "    \"\"\"\n",
    "    Wrapper around a DataLoader to also yield a task name\n",
    "    \"\"\"\n",
    "    def __init__(self, task_name, data_loader):\n",
    "        self.task_name = task_name\n",
    "        self.data_loader = data_loader\n",
    "\n",
    "        self.batch_size = data_loader.batch_size\n",
    "        self.dataset = data_loader.dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_loader)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.data_loader:\n",
    "            batch[\"task_name\"] = StrIgnoreDevice(self.task_name)\n",
    "            yield batch\n",
    "\n",
    "\n",
    "class MultitaskDataloader:\n",
    "    \"\"\"\n",
    "    Data loader that combines and samples from multiple single-task\n",
    "    data loaders.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataloader_dict):\n",
    "        self.dataloader_dict = dataloader_dict\n",
    "        self.num_batches_dict = {\n",
    "            task_name: len(dataloader) \n",
    "            for task_name, dataloader in self.dataloader_dict.items()\n",
    "        }\n",
    "        self.task_name_list = list(self.dataloader_dict)\n",
    "        self.dataset = [None] * sum(\n",
    "            len(dataloader.dataset) \n",
    "            for dataloader in self.dataloader_dict.values()\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(self.num_batches_dict.values())\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        For each batch, sample a task, and yield a batch from the respective\n",
    "        task Dataloader.\n",
    "\n",
    "        We use size-proportional sampling, but you could easily modify this\n",
    "        to sample from some-other distribution.\n",
    "        \"\"\"\n",
    "        task_choice_list = []\n",
    "        for i, task_name in enumerate(self.task_name_list):\n",
    "            task_choice_list += [i] * self.num_batches_dict[task_name]\n",
    "        task_choice_list = np.array(task_choice_list)\n",
    "        np.random.shuffle(task_choice_list)\n",
    "        dataloader_iter_dict = {\n",
    "            task_name: iter(dataloader) \n",
    "            for task_name, dataloader in self.dataloader_dict.items()\n",
    "        }\n",
    "        for task_choice in task_choice_list:\n",
    "            task_name = self.task_name_list[task_choice]\n",
    "            yield next(dataloader_iter_dict[task_name])    \n",
    "\n",
    "class MultitaskTrainer(transformers.Trainer):\n",
    "\n",
    "    def get_single_train_dataloader(self, task_name, train_dataset):\n",
    "        \"\"\"\n",
    "        Create a single-task data loader that also yields task names\n",
    "        \"\"\"\n",
    "        if self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "        if is_tpu_available():\n",
    "            train_sampler = get_tpu_sampler(train_dataset)\n",
    "        else:\n",
    "            train_sampler = (\n",
    "                RandomSampler(train_dataset)\n",
    "                if self.args.local_rank == -1\n",
    "                else DistributedSampler(train_dataset)\n",
    "            )\n",
    "\n",
    "        data_loader = DataLoaderWithTaskname(\n",
    "            task_name=task_name,\n",
    "            data_loader=DataLoader(\n",
    "              train_dataset,\n",
    "              batch_size=self.args.train_batch_size,\n",
    "              sampler=train_sampler,\n",
    "              collate_fn=self.data_collator.collate_batch,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if is_tpu_available():\n",
    "            data_loader = pl.ParallelLoader(\n",
    "                data_loader, [self.args.device]\n",
    "            ).per_device_loader(self.args.device)\n",
    "        return data_loader\n",
    "\n",
    "    def get_train_dataloader(self):\n",
    "        \"\"\"\n",
    "        Returns a MultitaskDataloader, which is not actually a Dataloader\n",
    "        but an iterable that returns a generator that samples from each \n",
    "        task Dataloader\n",
    "        \"\"\"\n",
    "        return MultitaskDataloader({\n",
    "            task_name: self.get_single_train_dataloader(task_name, task_dataset)\n",
    "            for task_name, task_dataset in self.train_dataset.items()\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started at  1608180744.130317\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c134bca69b54ebfac43b936c6f78c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=3.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe327f915c50461ea0cbfa327cfb6a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1868.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 1.3997068860381843, \"learning_rate\": 4.9107780157030696e-05, \"epoch\": 0.05353319057815846, \"step\": 100}\n",
      "{\"loss\": 1.368909692466259, \"learning_rate\": 4.821556031406139e-05, \"epoch\": 0.10706638115631692, \"step\": 200}\n",
      "{\"loss\": 1.0613712200522423, \"learning_rate\": 4.732334047109208e-05, \"epoch\": 0.16059957173447537, \"step\": 300}\n",
      "{\"loss\": 1.2776457597315312, \"learning_rate\": 4.643112062812277e-05, \"epoch\": 0.21413276231263384, \"step\": 400}\n",
      "{\"loss\": 1.1486877569556235, \"learning_rate\": 4.553890078515346e-05, \"epoch\": 0.2676659528907923, \"step\": 500}\n",
      "{\"loss\": 1.293742711544037, \"learning_rate\": 4.4646680942184155e-05, \"epoch\": 0.32119914346895073, \"step\": 600}\n",
      "{\"loss\": 1.1060688519477844, \"learning_rate\": 4.375446109921485e-05, \"epoch\": 0.3747323340471092, \"step\": 700}\n",
      "{\"loss\": 1.159701759070158, \"learning_rate\": 4.286224125624554e-05, \"epoch\": 0.4282655246252677, \"step\": 800}\n",
      "{\"loss\": 1.2830065727233886, \"learning_rate\": 4.1970021413276235e-05, \"epoch\": 0.4817987152034261, \"step\": 900}\n",
      "{\"loss\": 1.3130575159192086, \"learning_rate\": 4.107780157030692e-05, \"epoch\": 0.5353319057815846, \"step\": 1000}\n",
      "{\"loss\": 1.1281436049938203, \"learning_rate\": 4.0185581727337615e-05, \"epoch\": 0.588865096359743, \"step\": 1100}\n",
      "{\"loss\": 1.2665948814153671, \"learning_rate\": 3.9293361884368315e-05, \"epoch\": 0.6423982869379015, \"step\": 1200}\n",
      "{\"loss\": 1.197370194196701, \"learning_rate\": 3.8401142041399e-05, \"epoch\": 0.69593147751606, \"step\": 1300}\n",
      "{\"loss\": 1.2211370623111726, \"learning_rate\": 3.7508922198429695e-05, \"epoch\": 0.7494646680942184, \"step\": 1400}\n",
      "{\"loss\": 1.2038560795783997, \"learning_rate\": 3.661670235546039e-05, \"epoch\": 0.8029978586723768, \"step\": 1500}\n",
      "{\"loss\": 1.437634933590889, \"learning_rate\": 3.572448251249108e-05, \"epoch\": 0.8565310492505354, \"step\": 1600}\n",
      "{\"loss\": 1.4766811138391496, \"learning_rate\": 3.483226266952177e-05, \"epoch\": 0.9100642398286938, \"step\": 1700}\n",
      "{\"loss\": 1.094761699438095, \"learning_rate\": 3.394004282655247e-05, \"epoch\": 0.9635974304068522, \"step\": 1800}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe75d319cd8148f2a8ab6db51a6e4195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1868.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 1.2560767555236816, \"learning_rate\": 3.3047822983583155e-05, \"epoch\": 1.0171306209850106, \"step\": 1900}\n",
      "{\"loss\": 1.1454512107372283, \"learning_rate\": 3.215560314061385e-05, \"epoch\": 1.0706638115631693, \"step\": 2000}\n",
      "{\"loss\": 1.1601292730867863, \"learning_rate\": 3.126338329764454e-05, \"epoch\": 1.1241970021413277, \"step\": 2100}\n",
      "{\"loss\": 1.161359928548336, \"learning_rate\": 3.0371163454675235e-05, \"epoch\": 1.177730192719486, \"step\": 2200}\n",
      "{\"loss\": 1.1161352533102036, \"learning_rate\": 2.9478943611705928e-05, \"epoch\": 1.2312633832976445, \"step\": 2300}\n",
      "{\"loss\": 1.124098533987999, \"learning_rate\": 2.8586723768736618e-05, \"epoch\": 1.284796573875803, \"step\": 2400}\n",
      "{\"loss\": 1.1488444618880749, \"learning_rate\": 2.769450392576731e-05, \"epoch\": 1.3383297644539613, \"step\": 2500}\n",
      "{\"loss\": 1.1802565118670463, \"learning_rate\": 2.6802284082798e-05, \"epoch\": 1.39186295503212, \"step\": 2600}\n",
      "{\"loss\": 1.195280080586672, \"learning_rate\": 2.5910064239828698e-05, \"epoch\": 1.4453961456102784, \"step\": 2700}\n",
      "{\"loss\": 1.3199681517481805, \"learning_rate\": 2.5017844396859384e-05, \"epoch\": 1.4989293361884368, \"step\": 2800}\n",
      "{\"loss\": 1.1375019282102585, \"learning_rate\": 2.412562455389008e-05, \"epoch\": 1.5524625267665952, \"step\": 2900}\n",
      "{\"loss\": 1.1334854489564896, \"learning_rate\": 2.323340471092077e-05, \"epoch\": 1.6059957173447539, \"step\": 3000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ecbm4040/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 1.4074445095658303, \"learning_rate\": 2.2341184867951464e-05, \"epoch\": 1.6595289079229123, \"step\": 3100}\n",
      "{\"loss\": 1.2184402349591255, \"learning_rate\": 2.1448965024982158e-05, \"epoch\": 1.7130620985010707, \"step\": 3200}\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-22b8e889822c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m )\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m    469\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m                 \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m                 if (step + 1) % self.args.gradient_accumulation_steps == 0 or (\n",
      "\u001b[0;32m~/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_training_step\u001b[0;34m(self, model, inputs, optimizer)\u001b[0m\n\u001b[1;32m    581\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start=time()\n",
    "print('Training started at ',start)\n",
    "train_dataset = {\n",
    "    task_name: dataset[\"train\"] \n",
    "    for task_name, dataset in features_dict.items()\n",
    "}\n",
    "trainer = MultitaskTrainer(\n",
    "    model=multitask_model,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=\"./models/xlnet_third_run\",\n",
    "        overwrite_output_dir=True,\n",
    "        learning_rate=5e-5,\n",
    "        do_train=True,\n",
    "        num_train_epochs=3,\n",
    "        # Adjust batch size if this doesn't fit on the Colab GPU\n",
    "        per_device_train_batch_size=8,  \n",
    "        save_steps=3000,\n",
    "        logging_steps=100,\n",
    "        logging_dir='xlnet_logs_third_run'\n",
    "    ),\n",
    "    data_collator=NLPDataCollator(),\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "trainer.train()\n",
    "print(time()-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from time import time\n",
    "# start=time()\n",
    "# print('Training started at ',start)\n",
    "# train_dataset = {\n",
    "#     task_name: dataset[\"train\"] \n",
    "#     for task_name, dataset in features_dict.items()\n",
    "# }\n",
    "# trainer = MultitaskTrainer(\n",
    "#     model=multitask_model,\n",
    "#     args=transformers.TrainingArguments(\n",
    "#         output_dir=\"./models/multitask_model\",\n",
    "#         overwrite_output_dir=True,\n",
    "#         learning_rate=5e-5,\n",
    "#         do_train=True,\n",
    "#         num_train_epochs=3,\n",
    "#         # Adjust batch size if this doesn't fit on the Colab GPU\n",
    "#         per_device_train_batch_size=32,  \n",
    "#         save_steps=3000,\n",
    "#     ),\n",
    "#     data_collator=NLPDataCollator(),\n",
    "#     train_dataset=train_dataset,\n",
    "# )\n",
    "# trainer.train(optimizer='adamax')\n",
    "# print(time()-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((time()-start) / 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NLPDataCollator.collate_batch of <__main__.NLPDataCollator object at 0x7f50e8c02c40>>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32a659c5296f4f7598bc7ed7d7db51c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Validation: cola', max=131.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<bound method NLPDataCollator.collate_batch of <__main__.NLPDataCollator object at 0x7f50e8c02c40>>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb82ef5bad024f02961fd963634f278e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Validation: stsb', max=188.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<bound method NLPDataCollator.collate_batch of <__main__.NLPDataCollator object at 0x7f50e8c02c40>>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf7f0823b624248b283e37ac692d744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Validation: wnli', max=9.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preds_dict = {}\n",
    "for task_name in [\"cola\", \"stsb\", \"wnli\"]:\n",
    "    eval_dataloader = DataLoaderWithTaskname(\n",
    "        task_name,\n",
    "        trainer.get_eval_dataloader(eval_dataset=features_dict[task_name][\"validation\"])\n",
    "    )\n",
    "    print(eval_dataloader.data_loader.collate_fn)\n",
    "    preds_dict[task_name] = trainer._prediction_loop(\n",
    "        eval_dataloader, \n",
    "        description=f\"Validation: {task_name}\",\n",
    "    )\n",
    "    #print(task_name,preds_dict[task_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scipy\n",
      "  Downloading scipy-1.5.4-cp38-cp38-manylinux1_x86_64.whl (25.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.8 MB 4.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /home/ecbm4040/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages (from scipy) (1.19.2)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.23.2-cp38-cp38-manylinux1_x86_64.whl (6.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.8 MB 65.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /home/ecbm4040/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /home/ecbm4040/anaconda3/envs/mtxln-pytorch/lib/python3.8/site-packages (from scipy) (1.19.2)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1316 sha256=ffa0a9c3054b6baa6216e6ca0ec1a2f9259a3923e5293dceaed2ca24f7371cdd\n",
      "  Stored in directory: /home/ecbm4040/.cache/pip/wheels/22/0b/40/fd3f795caaa1fb4c6cb738bc1f56100be1e57da95849bfc897\n",
      "Successfully built sklearn\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn, sklearn\n",
      "Successfully installed scikit-learn-0.23.2 scipy-1.5.4 sklearn-0.0 threadpoolctl-2.1.0\n"
     ]
    }
   ],
   "source": [
    "! pip install scipy sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "\n",
    "scores['cola'] = nlp.load_metric('glue', name='cola').compute(\n",
    "    np.argmax(preds_dict['cola'].predictions, axis=1),\n",
    "    preds_dict['cola'].label_ids\n",
    ")\n",
    "scores['stsb'] = nlp.load_metric('glue', name=\"stsb\").compute(\n",
    "    preds_dict[\"stsb\"].predictions.flatten(),\n",
    "    preds_dict[\"stsb\"].label_ids,\n",
    ")\n",
    "scores['wnli'] = nlp.load_metric('glue', name='wnli').compute(\n",
    "    np.argmax(preds_dict['wnli'].predictions, axis=1),\n",
    "    preds_dict['wnli'].label_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cola': {'matthews_correlation': 0.002973634822921437},\n",
       " 'stsb': {'pearson': 0.8518357251907471, 'spearmanr': 0.852182635385541},\n",
       " 'wnli': {'accuracy': 0.5352112676056338}}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open('xlnet_scores_3.pkl', 'wb') as fd :\n",
    "    pickle.dump(scores, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-6.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-6:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
